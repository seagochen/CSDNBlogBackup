@[toc]

# Authors and Publishment

## Authors
* Florian Schroff / Google Inc.
* Dmitry Kalenichenko / Google Inc.
* James Philbin / Google Inc.


## Bibtex
Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering, Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823.

## Categories
Computer Graphics, Deep Learning, Object Detection


# 0.  Abstract
Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.

> 尽管人脸识别领域最近取得了重大进展 [10, 14, 15, 17]，但大规模有效地实施人脸验证和识别对当前方法提出了严峻挑战。 在本文中，我们提出了一个名为 FaceNet 的系统，它直接学习从人脸图像到紧凑欧几里得空间的映射，其中距离直接对应于人脸相似性的度量。 一旦产生了这个空间，就可以使用带有 FaceNet 嵌入作为特征向量的标准技术轻松实现人脸识别、验证和聚类等任务。

Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.

> 我们使用了经过训练的深度卷积网络来直接优化嵌入层本身，而不是像以前的深度学习方法那样使用中间瓶颈层。 为了训练，我们使用了使用一种新颖的在线三元组挖掘方法生成的粗略对齐的匹配/非匹配面块的三元组。 我们的方法的好处是更高的表示效率：我们仅使用每张脸 128 字节就实现了最先进的人脸识别性能。

On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [15] by 30% on both datasets.

> 在广泛使用的野外标记人脸 (LFW) 数据集上，我们的系统达到了 99.63% 的新记录准确率。 在 YouTube Faces DB 上，它达到了 95.12%。 与两个数据集上的最佳发布结果 [15] 相比，我们的系统将错误率降低了 30%。

# 1. Introduction
In this paper we present a unified system for face verification (is this the same person), recognition (who is this person) and clustering (find common people among these faces). Our method is based on learning a Euclidean embedding per image using a deep convolutional network. The network is trained such that the squared L2 distances in the embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances.

> 在本文中，我们提出了一个统一的系统，用于人脸验证（这是同一个人）、识别（这个人是谁）和聚类（在这些人脸中找到普通人）。 我们的方法基于使用深度卷积网络学习每张图像的欧几里得嵌入。 训练网络使得嵌入空间中的平方 L2 距离直接对应于人脸相似度：同一个人的人脸距离小，不同人的人脸距离大。

Once this embedding has been produced, then the aforementioned tasks become straight-forward: face verification simply involves thresholding the distance between the two embeddings; recognition becomes a k-NN classification problem; and clustering can be achieved using off-the- shelf techniques such as k-means or agglomerative clustering.

> 一旦产生了这个嵌入，那么前面提到的任务就变得简单了：人脸验证只涉及对两个嵌入之间的距离进行阈值化； 识别成为 k-NN 分类问题； 并且可以使用现成的技术（例如 k 均值或凝聚聚类）来实现聚类。

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/3307502b5a5e42af82a1e3a975998647.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Figure 1. Illumination and Pose invariance. Pose and illumination have been a long standing problem in face recognition. This figure shows the output distances of FaceNet between pairs of faces of the same and a different person in different pose and illumination combinations. A distance of 0.0 means the faces are identical, 4.0 corresponds to the opposite spectrum, two different identities. You can see that a threshold of 1.1 would classify every pair correctly.
> 译：
> 图 1. 光照和姿态不变性。 姿势和光照一直是人脸识别中长期存在的问题。 该图显示了不同姿势和光照组合下相同和不同人的成对人脸之间的 FaceNet 输出距离。 距离为 0.0 表示面部相同，4.0 对应于相反的光谱，两个不同的身份。 您可以看到 1.1 的阈值可以正确分类每一对。


Previous face recognition approaches based on deep networks use a classification layer [15, 17] trained over a set of known face identities and then take an intermediate bottleneck layer as a representation used to generalize recognition beyond the set of identities used in training. The downsides of this approach are its indirectness and its inefficiency: one has to hope that the bottleneck representation generalizes well to new faces; and by using a bottleneck layer the representation size per face is usually very large (1000s of dimensions). Some recent work [15] has reduced this dimensionality using PCA, but this is a linear transformation that can be easily learnt in one layer of the network.

> 先前基于深度网络的人脸识别方法使用在一组已知人脸身份上训练的分类层 [15, 17]，然后将中间瓶颈层作为表示用于泛化训练中使用的身份组之外的识别。 这种方法的缺点是它的间接性和低效率：人们必须希望瓶颈表示能够很好地推广到新面孔； 并且通过使用瓶颈层，每个面的表示尺寸通常非常大（1000 维）。 最近的一些工作 [15] 使用 PCA 降低了这个维度，但这是一种线性变换，可以在网络的一层中轻松学习。

In contrast to these approaches, FaceNet directly trains its output to be a compact 128-D embedding using a tripletbased loss function based on LMNN [19]. Our triplets consist of two matching face thumbnails and a non-matching face thumbnail and the loss aims to separate the positive pair from the negative by a distance margin. The thumbnails are tight crops of the face area, no 2D or 3D alignment, other than scale and translation is performed.

> 与这些方法相比，FaceNet 使用基于 LMNN [19] 的基于三元组的损失函数直接将其输出训练为紧凑的 128 维嵌入。 我们的三元组由两个匹配的人脸缩略图和一个不匹配的人脸缩略图组成，损失旨在通过距离间隔将正对与负对分开。 缩略图是面部区域的紧密裁剪，除了执行缩放和平移之外，没有 2D 或 3D 对齐。

Choosing which triplets to use turns out to be very important for achieving good performance and, inspired by curriculum learning [1], we present a novel online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains. To improve clustering accuracy, we also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person.

> 选择使用哪个三元组对于实现良好的表现非常重要，并且在课程学习 [1] 的启发下，我们提出了一种新颖的在线负样本挖掘策略，确保随着网络训练的不断增加三元组的难度。 为了提高聚类精度，我们还探索了硬正挖掘技术，这些技术鼓励用于单个人嵌入的球形集群。

As an illustration of the incredible variability that our method can handle see Figure 1. Shown are image pairs from PIE [13] that previously were considered to be very difficult for face verification systems.

> 作为我们的方法可以处理的令人难以置信的可变性的说明，请参见图 1。显示的是来自 PIE [13] 的图像对，这些图像对以前被认为对于人脸验证系统来说非常困难。

An overview of the rest of the paper is as follows: in section 2 we review the literature in this area; section 3.1 defines the triplet loss and section 3.2 describes our novel triplet selection and training procedure; in section 3.3 we describe the model architecture used. Finally in section 4 and 5 we present some quantitative results of our embeddings and also qualitatively explore some clustering results.

> 本文其余部分的概述如下：在第 2 节中，我们回顾了该领域的文献； 第 3.1 节定义了三元组损失，第 3.2 节描述了我们新颖的三元组选择和训练程序； 在 3.3 节中，我们描述了所使用的模型架构。 最后，在第 4 节和第 5 节中，我们展示了嵌入的一些定量结果，并定性地探索了一些聚类结果。

# 2. Related Work
Similarly to other recent works which employ deep networks [15, 17], our approach is a purely data driven method which learns its representation directly from the pixels of the face. Rather than using engineered features, we use a large dataset of labelled faces to attain the appropriate invariances to pose, illumination, and other variational conditions.

> 与最近使用深度网络的其他工作类似 [15, 17]，我们的方法是一种纯粹的数据驱动方法，它直接从人脸的像素中学习其表示。 我们没有使用工程特征，而是使用大量标记人脸数据集来获得姿势、光照和其他变化条件的适当不变性。

In this paper we explore two different deep network architectures that have been recently used to great success in the computer vision community. Both are deep convolutional networks [8, 11]. The first architecture is based on the Zeiler&Fergus [22] model which consists of multiple interleaved layers of convolutions, non-linear activations, local response normalizations, and max pooling layers. We additionally add several 1×1×d convolution layers inspired by the work of [9]. The second architecture is based on the Inception model of Szegedy et al. which was recently used as the winning approach for ImageNet 2014 [16]. These networks use mixed layers that run several different convolutional and pooling layers in parallel and concatenate their responses. We have found that these models can reduce the number of parameters by up to 20 times and have the potential to reduce the number of FLOPS required for comparable performance.

> 在本文中，我们探讨了最近在计算机视觉社区取得巨大成功的两种不同的深度网络架构。 两者都是深度卷积网络 [8, 11]。 第一种架构基于 Zeiler&Fergus [22] 模型，该模型由多个交错的卷积层、非线性激活、局部响应归一化和最大池化层组成。 我们额外添加了几个 1×1×d 卷积层，灵感来自 [9] 的工作。 第二种架构基于 Szegedy 等人的 Inception 模型。 它最近被用作 ImageNet 2014 [16] 的获胜方法。 这些网络使用混合层，这些层并行运行多个不同的卷积层和池化层，并将它们的响应连接起来。 我们发现这些模型可以将参数数量减少多达 20 倍，并且有可能减少可比性能所需的 FLOPS 数量。

There is a vast corpus of face verification and recognition works. Reviewing it is out of the scope of this paper so we will only briefly discuss the most relevant recent work.

> 有大量的人脸验证和识别作品。 回顾它超出了本文的范围，因此我们将只简要讨论最近最相关的工作。

The works of [15, 17, 23] all employ a complex system of multiple stages, that combines the output of a deep convolutional network with PCA for dimensionality reduction and an SVM for classification.

> [15,17,23] 的作品都采用了一个多阶段的复杂系统，将深度卷积网络的输出与用于降维的 PCA 和用于分类的 SVM 相结合。

Zhenyao et al. [23] employ a deep network to “warp” faces into a canonical frontal view and then learn CNN that classifies each face as belonging to a known identity. For face verification, PCA on the network output in conjunction with an ensemble of SVMs is used.

> 振耀等。 [23] 采用深度网络将人脸“扭曲”成规范的正面视图，然后学习 CNN 将每张脸分类为属于已知身份。 对于人脸验证，使用网络输出上的 PCA 与 SVM 集成。

Taigman et al. [17] propose a multi-stage approach that aligns faces to a general 3D shape model. A multi-class network is trained to perform the face recognition task on over four thousand identities. The authors also experimented with a so called Siamese network where they directly optimize the L1-distance between two face features. Their best performance on LFW (97.35%) stems from an ensemble of three networks using different alignments and color channels. The predicted distances (non-linear SVM predictions based on the χ2 kernel) of those networks are combined using a non-linear SVM.

> 泰格曼等人。 [17] 提出了一种多阶段方法，将人脸与一般 3D 形状模型对齐。 训练多类网络以对超过四千个身份执行人脸识别任务。 作者还对所谓的 Siamese 网络进行了实验，他们直接优化了两个面部特征之间的 L1 距离。 他们在 LFW 上的最佳性能 (97.35%) 源于使用不同对齐方式和颜色通道的三个网络的集成。 使用非线性 SVM 组合这些网络的预测距离（基于 χ2 核的非线性 SVM 预测）。

Sun et al. [14, 15] propose a compact and therefore relatively cheap to compute network. They use an ensemble of 25 of these network, each operating on a different face patch. For their final performance on LFW (99.47% [15]) the authors combine 50 responses (regular and flipped). Both PCA and a Joint Bayesian model [2] that effectively correspond to a linear transform in the embedding space are employed. Their method does not require explicit 2D/3D alignment. The networks are trained by using a combination of classification and verification loss. The verification loss is similar to the triplet loss we employ [12, 19], in that it minimizes the L2-distance between faces of the same identity and enforces a margin between the distance of faces of different identities. The main difference is that only pairs of images are compared, whereas the triplet loss encourages a relative distance constraint.

> 孙等人。 [14, 15] 提出了一种紧凑且因此相对便宜的计算网络。他们使用了 25 个这样的网络的集合，每个网络都在不同的面部补丁上运行。对于他们在 LFW (99.47% [15]) 上的最终表现，作者结合了 50 个响应（常规和翻转）。采用 PCA 和联合贝叶斯模型 [2]，它们有效地对应于嵌入空间中的线性变换。他们的方法不需要明确的 2D/3D 对齐。通过使用分类和验证损失的组合来训练网络。验证损失类似于我们使用的三元组损失 [12, 19]，因为它最小化了相同身份的人脸之间的 L2 距离，并在不同身份的人脸之间的距离之间强制执行了裕度。主要区别在于仅比较图像对，而三元组损失鼓励相对距离约束。

A similar loss to the one used here was explored in Wang et al. [18] for ranking images by semantic and visual similarity.

> Wang等人探索了与此处使用的损失类似的损失。 [18] 通过语义和视觉相似性对图像进行排序。


# 3. Method

FaceNet uses a deep convolutional network. We discuss two different core architectures: The Zeiler&Fergus [22] style networks and the recent Inception [16] type networks. The details of these networks are described in section 3.3.

> FaceNet 使用了深度卷积网络。我们讨论了两种不同的核心架构：Zeiler & Fergus [22] 的网络和最近的 Inception [16] 的网络。 这些网络的细节将在 3.3 节中描述。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/8e226234ab404b1b96559e0452f64dce.png#pic_center)
> Figure 2. Model structure. Our network consists of a batch input layer and a deep CNN followed by L2 normalization, which results in the face embedding. This is followed by the triplet loss during training.
> 译：
> 图2: 模型结构。 我们的网络由一个批量输入层和一个深度 CNN 组成，然后是 L2 归一化并用于人脸嵌入。接下来是训练期间的三元组损失。


Given the model details, and treating it as a black box (see Figure 2), the most important part of our approach lies in the end-to-end learning of the whole system. To this end we employ the triplet loss that directly reflects what we want to achieve in face verification, recognition and clustering. Namely, we strive for an embedding $f(x)$, from an image $x$ into a feature space $\mathbb{R}^d$, such that the squared distance between all faces, independent of imaging conditions, of the same identity is small, whereas the squared distance between a pair of face images from different identities is large.

> 给定模型细节将其视为黑盒（见图 2），我们方法中最重要的部分在于整个系统中端到端学习。 为此，我们采用了三元组损失，它直接反映了我们想要在人脸验证、识别和聚类中实现的目标。 即，我们将图像 $x$ 中的 $f(x)$ 嵌入到特征空间 $\mathbb{R}^d$ 中，使得所有面部之间的平方距离，与成像条件无关，相同的特征距离很小，而来自不同身份的人脸图像之间的特征距离很大。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/554cdbf17b4d4a639b4bb1be6719e440.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Figure 3. The Triplet Loss minimizes the distance between an anchor and a positive, both of which have the same identity, and maximizes the distance between the anchor and a negative of a different identity.
> 译：
> 图3:  anchor 和 positive 有相同的特征，和 negative 有不同的特征。Triplet Loss 在训练过程中会缩小 anchor 和 postive 的距离，并最大化 anchor 和 negative 的距离。


Although we did not a do direct comparison to other losses, e.g. the one using pairs of positives and negatives, as used in [14] Eq. (2), we believe that the triplet loss is more suitable for face verification. The motivation is that the loss from [14] encourages all faces of one identity to be a projected onto a single point in the embedding space. The triplet loss, however, tries to enforce a margin between each pair of faces from one person to all other faces. This allows the faces for one identity to live on a manifold, while still enforcing the distance and thus discriminability to other identities.

> 虽然我们没有比较其他损失，例如使用正负对的方法，在等式(2) [14] 中那样。我们认为三元组损失更适合人脸验证。之所以这样认为，是因为损失鼓励将所有面部的某个特征投影到嵌入空间的某个单点上 [14]。三元组损失会试图对每组人脸与其他人脸之间强制执行一个边距。这使得基于某类特征的人脸存在于一个流行上，同时仍保持距离，从而达到与其他身份进行区分的目的。

The following section describes this triplet loss and how it can be learned efficiently at scale.

> 接下来的章节详细的说明了三元组损失，以及怎么大规模有效地学习它。

## 3.1. Triplet Loss

The embedding is represented by $f(x) \in \mathbb{R}^d$ . It embeds an image $x$ into a d-dimensional Euclidean space. Additionally, we constrain this embedding to live on the d-dimensional hypersphere, i.e. $\left \| f(x) \right \|_2 = 1$. This loss is motivated in [19] in the context of nearest-neighbor classification. Here we want to ensure that an image $x_i^a (anchor)$ of a specific person is closer to all other images $x_i^p (positive)$ of the same person than it is to any image $x_i^n (negative)$ of any other person. This is visualized in Figure 3.

> 我们使用 $f(x) \in \mathbb{R}^d$ 表示嵌入。 它将图像 $x$ 嵌入到 $d$ 维欧几里得空间中。 此外，我们将这个嵌入限制在 $d$ 维超球面上，即 $\left \| f(x) \right \|_2 = 1$。 这种损失是在[19]中在最近邻分类的背景下导致的。 在这里，我们要确保特定人的图像 $x_i^a (anchor)$ 与同一人的所有其他图像 $x_i^p (positive)$ 相比，它比其他任何图像 $x_i^n (negative)$ 的距离更近。 这在图 3 中可视化。

Thus we want,

> 因此，我们希望

**Eq. 1**

$$
\left \|  x_i^a - x_i^p  \right \|_2^2 + \alpha < \left \|  x_i^a - x_i^n \right \|_2^2, \ \forall (x_i^a, x_i^p, x_i^n) \in \tau
$$

where $\alpha$ is a margin that is enforced between positive and negative pairs. $\tau$ is the set of all possible triplets in the training set and has cardinality $N$.

> 其中 $\alpha$ 是在正负对之间强制执行的边距。 $\tau$ 是训练集中所有可能的三元组的集合，具有基数 $N$。

The loss that is being minimized is then $L =$

> 需要求解的最小化损失为

**Eq. 2**

$$
\sum_{i}^{N} \left [  \left \|   f(x_{i}^{a}) -  f(x_{i}^{p})  \right \|_2^2  -   \left \|   f(x_{i}^{a}) -  f(x_{i}^{n})  \right \|_2^2  + \alpha \right ]
$$

Generating all possible triplets would result in many triplets that are easily satisfied (i.e. fulfill the constraint in Eq. (1)). These triplets would not contribute to the training and result in slower convergence, as they would still be passed through the network. It is crucial to select hard triplets, that are active and can therefore contribute to improving the model. The following section talks about the different approaches we use for the triplet selection.

> 生成所有可能的三元组（即满足等式（1）中的约束条件）。 这些三元组对训练没有贡献，并且会导致收敛速度变慢。所以，选择活跃的硬三元组至关重要，并有助于改进模型。 在下面的章节里，我们将探讨不同的三元组选择方法。


## 3.2. Triplet Selection

In order to ensure fast convergence it is crucial to select triplets that violate the triplet constraint in Eq. (1). This means that, given $x_i^a$ , we want to select an $x_i^p$ (hard positive) such that $argmax_{x_i^p} \left \| f (x_i^a ) − f (x_i^p ) \right \|_2^2$ and similarly $x_i^n$ (hard negative) such that $argmin_{x_i^n} \left \| f(x_i^a ) − f(x_i^n) \right \|_2^2$. 

> 为了确保快速收敛，选择违反方程式（1）中有关三元组的约束条件至关重要。这意味着，给定 $x_i^a$ ，我们要选择一个 $x_i^p$ （hard positive），使得 $argmax_{x_i^p} \left \| f (x_i^a ) − f (x_i^p ) \right \|_2^2$ 和类似的 $x_i^n$ （hard negative）使得 $argmin_{x_i^n} \left \| f(x_i^a ) − f(x_i^n) \right \|_2^2$。

It is infeasible to compute the argmin and argmax across the whole training set. Additionally, it might lead to poor training, as mislabelled and poorly imaged faces would dominate the hard positives and negatives. There are two obvious choices that avoid this issue:

> 在整个训练集中计算 argmin 和 argmax 是不可行的。 此外，它可能会导致训练不足，因为错误标记和图像不佳的人脸会影响hard positives 和 negatives。而我们有两个选择可以避免这个问题：

* Generate triplets offline every n steps, using the most recent network checkpoint and computing the argmin and argmax on a subset of the data.
* Generate triplets online. This can be done by selecting the hard positive/negative exemplars from within a mini-batch.

> * 每 n 步离线生成三元组，使用最近的网络检查点并在数据子集上计算 argmin 和 argmax。
> * 在线生成三元祖。 这可以通过从小批量中选择硬positive/negative样本来完成。

Here, we focus on the online generation and use large mini-batches in the order of a few thousand exemplars and only compute the argmin and argmax within a mini-batch.

> 在这篇文章里，我们专注于在线生成技术，并使用几千个样本量级的大型 mini-batch，并且只计算 mini-batch 中的 argmin 和 argmax。

To have a meaningful representation of the anchor-positive distances, it needs to be ensured that a minimal number of exemplars of any one identity is present in each mini-batch. In our experiments we sample the training data such that around 40 faces are selected per identity per mini-batch. Additionally, randomly sampled negative faces are added to each mini-batch.

> 为了对 anchor-positive 距离进行有意义的表示，需要确保每个小批量中存在最小数量的任何一个特征的样本。 在我们的实验中，我们对训练数据进行采样，使得每个小批量的每个特征包含大约 40 个人脸。 此外，随机采样的 negative 被添加到每个小批量中。

Instead of picking the hardest positive, we use all anchor-positive pairs in a mini-batch while still selecting the hard negatives. We don’t have a side-by-side comparison of hard anchor-positive pairs versus all anchor-positive pairs within a mini-batch, but we found in practice that the all anchor-positive method was more stable and converged slightly faster at the beginning of training.

> 我们没有选择采用 hardest positive，而是在一个小批量中使用所有 anchor-positive 对，同时仍然选择 hard negatives。 我们没有对小批量中的 hard anchor-positive 对与所有 anchor-positive 对逐一比较，但我们在实践中发现，anchor-positive 方法更稳定，从训练开始它的收敛速度更快。

We also explored the offline generation of triplets in conjunction with the online generation and it may allow the use of smaller batch sizes, but the experiments were inconclusive.

> 我们还探索了，结合了在线与离线生成三元组的方式，它可能允许使用较小的批量，但实验没有结论。

Selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x_i^n$ such that

> 在实践中，选择 hardest negatives 可能会在训练早期导致局部最小值，特别是它可能导致模型崩溃（即 $f(x) = 0$）。 为了减轻这种情况，可以选择 $x_i^n$ 使得

**Eq. 3**

$$
\left \|  f(x_i^a) - f(x_i^p) \right \|_2^2 < \left \| f(x_i^a) - f(x_i^n) \right \|_2^2
$$

We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchor-positive distance. Those negatives lie inside the margin $\alpha$.

> 我们称这些 negative 为 semi-hard 样本，因为它们比 positive 样本离锚点更远，但仍然很难，因为平方距离接近 anchor-positive 样本的距离。这些 negatives 样本位于 $\alpha$ 内。

As mentioned before, correct triplet selection is crucial for fast convergence. On the one hand we would like to use small mini-batches as these tend to improve convergence during Stochastic Gradient Descent (SGD) [20]. On the other hand, implementation details make batches of tens to hundreds of exemplars more efficient. The main constraint with regards to the batch size, however, is the way we select hard relevant triplets from within the mini-batches. In most experiments we use a batch size of around 1,800 exemplars.

> 如前所述，正确的三元组选择对于快速收敛至关重要。 一方面，我们希望使用小型 mini-batch，因为它们往往会在随机梯度下降 (SGD) [20] 期间提高收敛性。 另一方面，实现时使用数十到数百个样本的批次更有效率。然而，关于批量大小的主要限制是我们从小批量中选择硬相关三元组的方式。 在大多数实验中，我们使用大约 1,800 个样本的批量大小。

## 3.3. Deep Convolutional Networks
In all our experiments we train the CNN using Stochastic Gradient Descent (SGD) with standard backprop [8, 11] and AdaGrad [5]. In most experiments we start with a learning rate of 0.05 which we lower to finalize the model. The models are initialized from random, similar to [16], and trained on a CPU cluster for 1,000 to 2,000 hours. The decrease in the loss (and increase in accuracy) slows down drastically after 500h of training, but additional training can still significantly improve performance. The margin $\alpha$ is set to 0.2.

> 在我们所有的实验中，我们使用带有标准反向传播 [8, 11] 和 AdaGrad [5] 的随机梯度下降 (SGD) 来训练 CNN。 在大多数实验中，我们从 0.05 的学习率开始，并降低以最终确定模型。模型从随机初始化，类似于 [16]，并在 CPU 集群上训练 1,000 到 2,000 小时。训练 500 小时后，损失的减少（和准确性的增加）急剧下降，但额外的训练仍然可以显着提高性能。 边距 $\alpha$ 设置为 0.2。

We used two types of architectures and explore their trade-offs in more detail in the experimental section. Their practical differences lie in the difference of parameters and FLOPS. The best model may be different depending on the application. E.g. a model running in a datacenter can have many parameters and require a large number of FLOPS, whereas a model running on a mobile phone needs to have few parameters, so that it can fit into memory. All our models use rectified linear units as the non-linear activation function.

> 我们使用了两种类型的架构，并在实验部分更详细地探讨了它们的权衡。 它们的实际区别在于参数和 FLOPS 的不同。 最佳模型可能因应用而异。 例如。 在数据中心运行的模型可以有很多参数，需要大量的 FLOPS，而在手机上运行的模型需要很少的参数，以便它可以放入内存。 我们所有的模型都使用整流线性单元作为非线性激活函数。

The first category, shown in Table 1, adds $1 \times 1 \times d$ convolutional layers, as suggested in [9], between the standard convolutional layers of the Zeiler&Fergus [22] architecture and results in a model 22 layers deep. It has a total of 140 million parameters and requires around 1.6 billion FLOPS per image.

> 对于第一类，如表1所示，在 Zeiler&Fergus [22] 的结构里增加了 $1 \times 1 \times d$ 卷积层，如[9]所建议的那样，产生了 22 层深的模型。它总共有 1.4 亿个参数，每张图像需要大约 16 亿次 FLOPS。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/6aa3df0c5c4f4de68394a05cc9885625.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Table 1. NN1. This table show the structure of our Zeiler&Fergus [22] based model with $1 \times 1$ convolutions in- spired by [9]. The input and output sizes are described in $rows \times cols \times filters$. The kernel is specified as $rows \times cols$, stride and the maxout [6] pooling size as $p = 2$.
> 译：
> 表 1. NN1。 该表显示了我们基于 Zeiler&Fergus [22] 的模型的结构，该模型具有受 [9] 启发的 $1 \times 1$ 卷积。 输入和输出大小以 $rows \times cols \times filters$ 描述。 内核被指定为 $rows \times cols$、stride 和 maxout [6] 池大小为 $p = 2$。

The second category we use is based on GoogLeNet style Inception models [16]. These models have $20 \times$ fewer parameters (around 6.6M-7.5M) and up to 5× fewer FLOPS (between 500M-1.6B). Some of these models are dramatically reduced in size (both depth and number of filters), so that they can be run on a mobile phone. One, NNS1, has 26M parameters and only requires 220M FLOPS per image. The other, NNS2, has 4.3M parameters and 20M FLOPS. Table 2 describes NN2 our largest network in detail. NN3 is identical in architecture but has a reduced input size of $160 \times 160$. NN4 has an input size of only $96 \times 96$, thereby drastically reducing the CPU requirements (285M FLOPS vs 1.6B for NN2). In addition to the reduced input size it does not use $5 \times 5$ convolutions in the higher layers as the receptive field is already too small by then. Generally we found that the $5 \times 5$ convolutions can be removed throughout with only a minor drop in accuracy. Figure 4 compares all our models.

> 第二类，我们基于GoogLeNet风格的Inception模型[16]。这些模型的参数减少了约20倍（大约 6.6M-7.5M），FLOPS 减少了 5 倍。其中一些模型的尺寸（深度和过滤器数量）显着减小，因此它们可以在手机上运行。One, NNS1，有 26M 参数，每张图像只需要 220M FLOPS。另一个，NNS2，有 4.3M 参数和 20M FLOPS。表 2 详细描述了我们最大的网络 NN2。 NN3 在架构上是相同的，但输入大小减少了 $160 \times 160$。 NN4 的输入大小仅为 $96 \times 96$，从而大大降低了 CPU 需求（285M FLOPS 与 NN2 的 1.6B）。除了减少输入大小之外，它没有在较高层中使用 $5 \times 5$ 的卷积，因为可接收范围已经太小了。一般来说，我们发现 $5 × 5$ 的卷积可以在整个过程中被移除，而准确度只会有轻微的下降。图 4 比较了我们所有的模型。


>![在这里插入图片描述](https://img-blog.csdnimg.cn/d0d9da264fd94c059e9ee9de5601f38a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Table 2. NN2. Details of the NN2 Inception incarnation. This model is almost identical to the one described in [16]. The two major differences are the use of L2 pooling instead of max pooling (m), where specified. The pooling is always $3 \times 3$ (aside from the final average pooling) and in parallel to the convolutional modules inside each Inception module. If there is a dimensionality reduction after the pooling it is denoted with $p$. $1 \times 1$, $3 \times 3$, and $5 \times 5$ pooling are then concatenated to get the final output.
> 译：
> 表2. NN2. NN2 Inception的详细信息。该模型和[16]中描述的很相似。最主要的两个区别是在L2用池化方法替代了最大池化方法。池化总是$3 \times 3$大小（除了最终的平均池化）并与每个 Inception 模块内的卷积模块并行。 如果在池化之后有降维，则用 $p$ 表示。 然后将 $1\times 1$、$3\times 3$ 和 $5\times 5$ 池连接起来以获得最终输出。


# 4. Datasets and Evaluation

We evaluate our method on four datasets and with the exception of Labelled Faces in the Wild and YouTube Faces we evaluate our method on the face verification task. I.e. given a pair of two face images a squared L2 distance threshold $D(x_i, x_j)$ is used to determine the classification of same and different. All faces pairs $(i, j)$ of the same identity are denoted with $P_{same}$, whereas all pairs of different identities are denoted with $P_{diff}$.

> 我们在四个数据集上评估我们的方法，除了野外标记的面孔和 YouTube 面孔之外，我们在人脸验证任务上评估我们的方法。 例如，给定一对两张人脸图像，使用平方 L2 距离阈值 $D(x_i, x_j)$ 来确定相同和不同的分类。 所有具有相同身份的人脸对 $(i, j)$ 用 $P_{same}$ 表示，而所有不同身份的人脸对用 $P_{diff}$ 表示。

We define the set of all *true accepts* as

> 我们定义 *true accepts* 为

**Eq. 4**

$$
TA(d) = \{  (i, j) \in P_{same}, \ with \ D(x_i, x_j) \leq d \}
$$

These are the face pairs $(i, j)$ that were correctly classified as same at threshold d. Similarly

> 这些是在阈值 d 被正确分类为相同的面部对 $(i, j)$。 相似地

**Eq. 5**

$$
FA(d) = \{  (i, j) \in P_{diff}, \ with \ D(x_i, x_j) \leq d \}
$$

is the set of all pairs that was incorrectly classified as same (false accept).

> 是被错误分类为相同（错误接受）的所有对的集合。

The validation rate VAL(d) and the false accept rate FAR(d) for a given face distance d are then defined as

> 然后将给定面部距离 d 的验证率 VAL(d) 和错误接受率 FAR(d) 定义为

**Eq. 6**

$$
VAL(d) = \frac{| TA(d) |}{| P_{same} |}, \ FAR(d) = \frac{| FA(d) |}{| P_{diff} |}
$$


## 4.1. Hold-out Test Set

We keep a hold out set of around one million images, that has the same distribution as our training set, but dis- joint identities. For evaluation we split it into five disjoint sets of 200k images each. The FAR and VAL rate are then computed on $100k \times 100k$ image pairs. Standard error is reported across the five splits.

> 我们保留了一组大约一百万张图像，与我们的训练集具有相同的分布，但身份不同。 为了评估，我们将其分成五组不相交的图像，每组 20 万张图像。 然后在 $100k \times 100k$ 图像对上计算 FAR 和 VAL 率。 在五个拆分中报告标准误差。

## 4.2. Personal Photos

Thisisatestsetwithsimilardistributiontoourtraining set, but has been manually verified to have very clean labels. It consists of three personal photo collections with a total of around 12k images. We compute the FAR and VAL rate across all 12k squared pairs of images.

> 这是一个与我们的训练集分布相似的测试集，但已经过手动验证，标签非常干净。 它由三个个人照片集组成，共有约 12k 张图像。 我们计算所有 12k 平方图像对的 FAR 和 VAL 率。

## 4.3. Academic Datasets

Labeled Faces in the Wild (LFW) is the de-facto aca- demic test set for face verification [7]. We follow the stan- dard protocol for unrestricted, labeled outside data and re- port the mean classification accuracy as well as the standard error of the mean.

> Labeled Faces in the Wild (LFW) 是用于人脸验证的事实上的学术测试集 [7]。 我们遵循不受限制的、标记的外部数据的标准协议，并报告平均分类准确度以及均值的标准误差。

Youtube Faces DB [21] is a new dataset that has gained popularity in the face recognition community [17, 15]. The setup is similar to LFW, but instead of verifying pairs of images, pairs of videos are used.

> Youtube Faces DB [21] 是一个在人脸识别社区 [17, 15] 中广受欢迎的新数据集。 该设置类似于 LFW，但不是验证成对的图像，而是使用成对的视频。


# 5. Experiments

If not mentioned otherwise we use between 100M-200M training face thumbnails consisting of about 8M different identities. A face detector is run on each image and a tight bounding box around each face is generated. These face thumbnails are resized to the input size of the respective network. Input sizes range from $96 \times 96$ pixels to $224 \times 224$ pixels in our experiments.

> 如果没有另外提及，我们使用 100M-200M 的训练人脸缩略图，其中包含大约 8M 不同的身份。 在每个图像上运行一个人脸检测器，并在每个人脸周围生成一个紧密的边界框。 这些人脸缩略图的大小被调整为相应网络的输入大小。 在我们的实验中，输入尺寸范围从 $96 \times 96$ 像素到 $224 \times 224$ 像素。

## 5.1. Computation Accuracy Trade-off

Before diving into the details of more specific experiments lets discuss the trade-off of accuracy versus number of FLOPS that a particular model requires. Figure 4

> 在深入了解更具体实验的细节之前，让我们讨论一下特定模型所需的准确性与 FLOPS 数量之间的权衡。 图 4

>![在这里插入图片描述](https://img-blog.csdnimg.cn/dce8ad9cc9d24aae9e5d578937e71ea9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Figure 4. FLOPS vs. Accuracy trade-off. Shown is the trade-off between FLOPS and accuracy for a wide range of different model sizes and architectures. Highlighted are the four models that we focus on in our experiments.
> 图 4. FLOPS 与准确性的权衡。 显示的是各种不同模型大小和架构的 FLOPS 和准确性之间的权衡。 突出显示的是我们在实验中关注的四个模型。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/2f59289e748b47c882a9a7d86c14b9b5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Table 3. Network Architectures. This table compares the per- formance of our model architectures on the hold out test set (see section 4.1). Reported is the mean validation rate VAL at 10E-3 false accept rate. Also shown is the standard error of the mean across the five test splits.
> 表 3. 网络架构。 该表比较了我们的模型架构在保持测试集上的性能（参见第 4.1 节）。 报告的是 10E-3 错误接受率的平均验证率 VAL。 还显示了五个测试拆分的平均值的标准误差。


shows the FLOPS on the x-axis and the accuracy at 0.001 false accept rate (FAR) on our user labelled test-data set from section 4.2. It is interesting to see the strong correlation between the computation a model requires and the accuracy it achieves. The figure highlights the five models (NN1, NN2, NN3, NNS1, NNS2) that we discuss in more detail in our experiments.

> 显示了 x 轴上的 FLOPS 和 0.001 错误接受率 (FAR) 在我们的用户标记的测试数据集（第 4.2 节）的准确度。 有趣的是，模型所需的计算量与其所达到的准确度之间存在很强的相关性。 该图突出显示了我们在实验中更详细讨论的五个模型（NN1、NN2、NN3、NNS1、NNS2）。

We also looked into the accuracy trade-off with regards to the number of model parameters. However, the picture is not as clear in that case. For example, the Inception based model NN2 achieves a comparable performance to NN1, but only has a 20th of the parameters. The number of FLOPS is comparable, though. Obviously at some point the performance is expected to decrease, if the number of parameters is reduced further. Other model architectures may allow further reductions without loss of accuracy, just like Inception [16] did in this case.

> 我们还研究了关于模型参数数量的准确性权衡。 但是，在这种情况下，情况并不那么清晰。 例如，基于 Inception 的模型 NN2 实现了与 NN1 相当的性能，但只有 20th 的参数。 不过，FLOPS 的数量是可比的。 显然，如果参数数量进一步减少，性能预计会下降。 其他模型架构可能允许进一步减少而不损失准确性，就像 Inception [16] 在这种情况下所做的那样。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/898dbf78d97644e0aa23cc07045585fe.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Figure 5. Network Architectures. This plot shows the com- plete ROC for the four different models on our personal pho- tos test set from section 4.2. The sharp drop at 10E-4 FAR can be explained by noise in the groundtruth labels. The mod- els in order of performance are: NN2: $224 \times 224$ input Inception based model; NN1: Zeiler&Fergus based network with $1 \times 1$ convolutions; NNS1: small Inception style model with only 220M FLOPS; NNS2: tiny Inception model with only 20M FLOPS.
> 图 5. 网络架构。 该图显示了第 4.2 节中我们的个人照片测试集上四种不同模型的完整 ROC。 10E-4 FAR 的急剧下降可以用 groundtruth 标签中的噪声来解释。 模型按性能排序为： NN2：$224 \times 224$ 输入基于 Inception 的模型； NN1：基于 Zeiler&Fergus 的 $1 \times 1$ 卷积网络； NNS1：只有 220M FLOPS 的小型 Inception 风格模型； NNS2：只有 20M FLOPS 的小型 Inception 模型。


## 5.2. Effect of CNN Model

We now discuss the performance of our four selected models in more detail. On the one hand we have our tradi- tional Zeiler&Fergus based architecture with $1 \times 1$ convolutions [22, 9] (see Table 1). On the other hand we have Inception [16] based models that dramatically reduce the model size. Overall, in the final performance the top models of both architectures perform comparably. However, some of our Inception based models, such as NN3, still achieve good performance while significantly reducing both the FLOPS and the model size.

> 我们现在更详细地讨论我们选择的四个模型的性能。 一方面，我们有传统的基于 Zeiler&Fergus 的架构，具有 $1 \times 1$ 卷积 [22, 9]（见表 1）。 另一方面，我们有基于 Inception [16] 的模型，可以显着减小模型大小。 总体而言，在最终性能中，两种架构的顶级模型的性能相当。 然而，我们的一些基于 Inception 的模型，例如 NN3，仍然取得了良好的性能，同时显着降低了 FLOPS 和模型大小。

The detailed evaluation on our personal photos test set is shown in Figure 5. While the largest model achieves a dramatic improvement in accuracy compared to the tiny NNS2, the latter can be run 30ms / image on a mobile phone and is still accurate enough to be used in face clustering. The sharp drop in the ROC for FAR < 10−4 indicates noisy labels in the test data groundtruth. At extremely low false accept rates a single mislabeled image can have a significant impact on the curve.

> 我们的个人照片测试集的详细评估如图 5 所示。虽然最大的模型与微型 NNS2 相比在准确度上取得了显着提升，但后者在手机上可以运行 30ms/张，仍然足够准确 用于人脸聚类。 FAR < 10-4 的 ROC 急剧下降表明测试数据中的标签有噪声。 在极低的错误接受率下，单个错误标记的图像可能会对曲线产生重大影响。


## 5.3. Sensitivity to Image Quality

Table 4 shows the robustness of our model across a wide range of image sizes. The network is surprisingly robust with respect to JPEG compression and performs very well down to a JPEG quality of 20. The performance drop is very small for face thumbnails down to a size of $120 \times 120$ pixels and even at $80 \times 80$ pixels it shows acceptable performance. This is notable, because the network was trained on $220 \times 220$ input images. Training with lower resolution faces could improve this range further.

> 表 4 显示了我们的模型在各种图像尺寸下的鲁棒性。 该网络在 JPEG 压缩方面出人意料地健壮，并且在 JPEG 质量为 20 时表现非常出色。对于尺寸为 $120 \times 120$ 像素的面部缩略图，性能下降非常小，即使在 $80 \times 80$ 像素时也显示出可接受的性能。 这是值得注意的，因为该网络是在 $220 \times 220$ 输入图像上训练的。 用较低分辨率的人脸训练可以进一步提高这个范围。

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/144991862e964160accef037568847cb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Table 4. Image Quality. The table on the left shows the effect on the validation rate at 10E-3 precision with varying JPEG quality. The one on the right shows how the image size in pixels effects the validation rate at 10E-3 precision. This experiment was done with NN1 on the first split of our test hold-out dataset.
> 表 4. 图像质量。 左侧的表格显示了 10E-3 精度下不同 JPEG 质量对验证率的影响。 右边的一个显示了以像素为单位的图像大小如何影响 10E-3 精度的验证率。 该实验是在我们的测试保留数据集的第一次拆分上使用 NN1 完成的。

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/ea52e122984d47b4b0aa69c8bfd328d4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Table 5. Embedding Dimensionality. This Table compares the effect of the embedding dimensionality of our model NN1 on our hold-out set from section 4.1. In addition to the VAL at 10E-3 we also show the standard error of the mean computed across five splits.
> 表 5. 嵌入维度。 该表比较了我们的模型 NN1 的嵌入维度对第 4.1 节中的保留集的影响。 除了 10E-3 处的 VAL，我们还显示了跨五个拆分计算的平均值的标准误差。

## 5.4.  Embedding Dimensionality
We explored various embedding dimensionalities and se- lected 128 for all experiments other than the comparison re- ported in Table 5. One would expect the larger embeddings to perform at least as good as the smaller ones, however, it is possible that they require more training to achieve the same accuracy. That said, the differences in the performance re- ported in Table 5 are statistically insignificant.

> 除了表 5 中报告的比较之外，我们探索了各种嵌入维度并为所有实验选择了 128 个。人们期望较大的嵌入至少与较小的嵌入一样好，但是，它们可能需要更多 训练以达到相同的精度。 也就是说，表 5 中报告的性能差异在统计上不显着。

It should be noted, that during training a 128 dimensional float vector is used, but it can be quantized to 128-bytes without loss of accuracy. Thus each face is compactly represented by a 128 dimensional byte vector, which is ideal for large scale clustering and recognition. Smaller embed- dings are possible at a minor loss of accuracy and could be employed on mobile devices.

> 需要注意的是，在训练过程中使用了 128 维浮点向量，但它可以量化为 128 字节而不会损失精度。 因此，每个人脸都由一个 128 维字节向量紧凑地表示，这是大规模聚类和识别的理想选择。 较小的嵌入是可能的，但精度损失很小，并且可以在移动设备上使用。

## 5.5. Amount of Training Data

Table 6 shows the impact of large amounts of training data. Due to time constraints this evaluation was run on a smaller model; the effect may be even larger on larger models. It is clear that using tens of millions of exemplars results in a clear boost of accuracy on our personal photo test set from section 4.2. Compared to only millions of images the relative reduction in error is 60%. Using another order of magnitude more images (hundreds of millions) still gives a small boost, but the improvement tapers off.

> 表 6 显示了大量训练数据的影响。 由于时间限制，此评估在较小的模型上运行； 在更大的模型上效果可能更大。 很明显，使用数以千万计的样本可以明显提高第 4.2 节中我们的个人照片测试集的准确性。 与仅数百万张图像相比，误差相对减少了 60%。 使用另一个数量级的更多图像（数亿）仍然会带来小幅提升，但改进会逐渐减弱。

> ![在这里插入图片描述](https://img-blog.csdnimg.cn/77d3a66132bc437594e2eab176449fc0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Table 6. Training Data Size. This table compares the performance after 700h of training for a smaller model with $96 \times 96$ pixel inputs. The model architecture is similar to NN2, but without the $5 \times 5$ con- volutions in the Inception modules.
> 表 6. 训练数据大小。 此表比较了具有 $96 \times 96$ 像素输入的较小模型在训练 700 小时后的性能。 模型架构类似于 NN2，但在 Inception 模块中没有 $5 \times 5$ 卷积。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/56348f1775244be88e13e8bd4577ccdd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Figure 6. LFW errors. This shows all pairs of images that were incorrectly classified on LFW.
> 图 6. LFW 错误。 这显示了在 LFW 上被错误分类的所有图像对。


## 5.6. Performance on LFW

We evaluate our model on LFW using the standard pro- tocol for unrestricted, labeled outside data. Nine training splits are used to select the L2-distance threshold. Classification (same or different) is then performed on the tenth test split. The selected optimal threshold is 1.242 for all test splits except split eighth (1.256).

> 我们使用标准协议评估我们在 LFW 上的模型，用于不受限制的、标记的外部数据。 九个训练分割用于选择 L2 距离阈值。 然后在第十个测试拆分上执行分类（相同或不同）。 对于除八分之一 (1.256) 之外的所有测试拆分，选定的最佳阈值为 1.242。

Our model is evaluated in two modes:

1. Fixed center crop of the LFW provided thumbnail.
2. Aproprietaryfacedetector(similartoPicasa[3])isrun on the provided LFW thumbnails. If it fails to align the face (this happens for two images), the LFW alignment is used.

> 我们的模型以两种模式进行评估：
> 1. 修复了 LFW 提供的缩略图的中心裁剪。
> 2. 在提供的 LFW 缩略图上运行专有的人脸检测器（类似于 Picasa[3]）。 如果它未能对齐面部（这发生在两个图像上），则使用 LFW 对齐。

Figure 6 gives an overview of all failure cases. It shows false accepts on the top as well as false rejects at the bottom. We achieve a classification accuracy of $98.87 \% \pm 0.15$ when using the fixed center crop described in (1) and the record breaking $99.63 \% \pm 0.09$ standard error of the mean when using the extra face alignment (2). This reduces the error reported for DeepFace in [17] by more than a factor of 7 and the previous state-of-the-art reported for DeepId2+ in [15] by $30 \%$. This is the performance of model NN1, but even the much smaller NN3 achieves performance that is not statistically significantly different.


> 图 6 给出了所有失败案例的概览。 它在顶部显示错误接受，在底部显示错误拒绝。 当使用 (1) 中描述的固定中心裁剪时，我们实现了 $98.87 \% \pm 0.15$ 的分类准确度，而在使用额外人脸对齐 (2) 时，我们达到了打破平均标准误差的 $99.63 \% \pm 0.09$ 的记录。 这将 [17] 中 DeepFace 报告的错误减少了 7 倍以上，并且将 [15] 中 DeepId2+ 报告的先前最先进技术减少了 $30 \%$。 这是模型 NN1 的性能，但即使是更小的 NN3 也能实现在统计上没有显着差异的性能。

## 5.7. Performance on Youtube Faces DB

We use the average similarity of all pairs of the first one hundred frames that our face detector detects in each video. This gives us a classification accuracy of $95.12 \% \pm 0.39$. Using the first one thousand frames results in $95.18 \%$. Compared to [17] $91.4 \%$ who also evaluate one hundred frames per video we reduce the error rate by almost half. DeepId2+ [15] achieved $93.2 \%$ and our method reduces this error by $30 \%$, comparable to our improvement on LFW.

> 我们使用人脸检测器在每个视频中检测到的前一百帧的所有对的平均相似度。 这为我们提供了 $95.12 \% \pm 0.39$ 的分类准确率。 使用前一千帧的结果是 $95.18 \%$。 与同样评估每个视频一百帧的 [17] $91.4 \%$ 相比，我们将错误率降低了近一半。 DeepId2+ [15] 达到了 $93.2 \%$，我们的方法将此误差降低了 $30 \%$，与我们对 LFW 的改进相当。


## 5.8. Face Clustering

Our compact embedding lends itself to be used in order to cluster a users personal photos into groups of people with the same identity. The constraints in assignment imposed by clustering faces, compared to the pure verification task, lead to truly amazing results. Figure 7 shows one cluster in a users personal photo collection, generated using agglom- erative clustering. It is a clear showcase of the incredible invariance to occlusion, lighting, pose and even age.

> 我们的紧凑嵌入有助于将用户的个人照片聚集到具有相同身份的人群中。 与纯验证任务相比，聚类人脸所施加的分配约束会产生真正惊人的结果。 图 7 显示了用户个人照片集中的一个集群，它是使用聚合聚类生成的。 它清楚地展示了对遮挡、照明、姿势甚至年龄的难以置信的不变性。

# 6. Summary

We provide a method to directly learn an embedding into an Euclidean space for face verification. This sets it apart from other methods [15, 17] who use the CNN bottleneck layer, or require additional post-processing such as concate- nation of multiple models and PCA, as well as SVM clas- sification. Our end-to-end training both simplifies the setup and shows that directly optimizing a loss relevant to the task at hand improves performance.

> 我们提供了一种直接学习嵌入到欧几里得空间中进行人脸验证的方法。 这使它与其他使用 CNN 瓶颈层的方法 [15, 17] 区别开来，或者需要额外的后处理，例如多个模型和 PCA 的串联，以及 SVM 分类。 我们的端到端训练既简化了设置，又表明直接优化与手头任务相关的损失可以提高性能。 

>![在这里插入图片描述](https://img-blog.csdnimg.cn/18906728e3b64cbab5132b127107a265.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Figure 7. Face Clustering. Shown is an exemplar cluster for one user. All these images in the users personal photo collection were clustered together.
> 图 7. 人脸聚类。 显示的是一个用户的示例集群。 用户个人照片集中的所有这些图像都聚集在一起。


Another strength of our model is that it only requires minimal alignment (tight crop around the face area). [17], for example, performs a complex 3D alignment. We also experimented with a similarity transform alignment and no- tice that this can actually improve performance slightly. It is not clear if it is worth the extra complexity.

> 我们模型的另一个优点是它只需要最小的对齐（面部区域周围的紧密裁剪）。 [17]，例如，执行复杂的 3D 对齐。 我们还尝试了相似变换对齐，并注意到这实际上可以稍微提高性能。 目前尚不清楚是否值得额外的复杂性。

Future work will focus on better understanding of the error cases, further improving the model, and also reducing model size and reducing CPU requirements. We will also look into ways of improving the currently extremely long training times, e.g. variations of our curriculum learn- ing with smaller batch sizes and offline as well as online positive and negative mining.

> 未来的工作将集中在更好地理解错误情况，进一步改进模型，以及减少模型大小和降低 CPU 需求。 我们还将研究改善目前极长的训练时间的方法，例如 我们的课程学习的变化与较小的批量和离线以及在线正负挖掘。

# Acknowledgments

We would like to thank Johannes Steffens for his discus- sions and great insights on face recognition and Christian Szegedy for providing new network architectures like [16] and discussing network design choices. Also we are in- debted to the DistBelief [4] team for their support espe- cially to Rajat Monga for help in setting up efficient training schemes.

Also our work would not have been possible without the support of Chuck Rosenberg, Hartwig Adam, and Simon Han.


# References
[1] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Cur- riculum learning. In Proc. of ICML, New York, NY, USA, 2009. 2
[2] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face revisited: A joint formulation. In Proc. ECCV, 2012. 2
[3] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun. Joint cascade
face detection and alignment. In Proc. ECCV, 2014. 8
[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Wein-
berger, editors, NIPS, pages 1232–1240. 2012. 9
[5] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. J.
Mach. Learn. Res., 12:2121–2159, July 2011. 4
[6] I. J. Goodfellow, D. Warde-farley, M. Mirza, A. Courville,
and Y. Bengio. Maxout networks. In In ICML, 2013. 4
[7] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Re- port 07-49, University of Massachusetts, Amherst, October
2007. 5
[8] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Compu- tation, 1(4):541–551, Dec. 1989. 2, 4
[9] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013. 2, 4, 6
[10] C. Lu and X. Tang. Surpassing human-level face veri- fication performance on LFW with gaussianface. CoRR, abs/1404.3840, 2014. 1
[11] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 1986. 2, 4
[12] M.SchultzandT.Joachims.Learningadistancemetricfrom relative comparisons. In S. Thrun, L. Saul, and B. Schölkopf, editors, NIPS, pages 41–48. MIT Press, 2004. 2
[13] T.Sim,S.Baker,andM.Bsat.TheCMUpose,illumination, and expression (PIE) database. In In Proc. FG, 2002. 2
[14] Y. Sun, X. Wang, and X. Tang. Deep learning face
representation by joint identification-verification. CoRR, abs/1406.4773, 2014. 1, 2, 3
[15] Y. Sun, X. Wang, and X. Tang. Deeply learned face representations are sparse, selective, and robust. CoRR, abs/1412.1265, 2014. 1, 2, 5, 8
[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.2,4,5,6,9
[17] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verifica- tion. In IEEE Conf. on CVPR, 2014. 1, 2, 5, 8
[18] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen, and Y. Wu. Learning fine-grained image similarity with deep ranking. CoRR, abs/1404.4661, 2014. 2
[19] K.Q.Weinberger,J.Blitzer,andL.K.Saul.Distancemetric learning for large margin nearest neighbor classification. In NIPS. MIT Press, 2006. 2, 3
[20] D. R. Wilson and T. R. Martinez. The general inefficiency of batch training for gradient descent learning. Neural Net- works, 16(10):1429–1451, 2003. 4
[21] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un- constrained videos with matched background similarity. In IEEE Conf. on CVPR, 2011. 5
[22] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. 2, 4, 6 [23] Z. Zhu, P. Luo, X. Wang, and X. Tang. Recover canonical- view faces in the wild with deep neural networks. CoRR,
abs/1404.3543, 2014. 2