#  Authors and Publishment

## Authors

* Francisco S. Melo / Institute for Systems and Robotics, Lisboa, PORTUGAL

## Bibtex
Melo F S. Convergence of Q-learning: A simple proof[J]. Institute Of Systems and Robotics, Tech. Rep, 2001: 1-4.


# 1 Preliminaries
We denote a Markov decision process as a tuple $(X , A, P, r)$, where:

> 我们可以将马尔可夫决策过程表示为元组 $(X, A, P, r)$，其中：

* $X$ is the (finite) state-space;
* $A$ is the (finite) action-space;
* $P$ represents the transition probabilities;
* $r$ represents the reward function.

We denote elements of $X$ as $x$ and $y$ and elements of $A$ as $a$ and $b$. We admit the general situation where the reward is defined over triplets $(x, a, y)$, i.e., $r$ is a function

> 将「有限状态空间」 $X$ 的表示为 $x$ 和 $y$，「有限动作空间」$A$ 表示为 $a$ 和 $b$。 对于通常情况，我们定义奖励函数表示为一个三元组： $(x, a, y)$ ，并表示为：

$$
r : X \times A \times X \to R
$$

assigning a reward $r(x,a,y)$ everytime a transition from $x$ to $y$ occurs due to action $a$. We admit $r$ to be a bounded, deterministic function.

> 状态经由动作 $a$ ，实现从 $x$ 到 $y$ 的转换时，获得奖励 $r(x,a,y)$。 同时奖励函数 $r$ 是一个有界的确定性函数。

The value of a state $x$ is defined, for a sequence of controls $\{ A_t \}$, as

> 状态 $x$ 的值被定义为一系列控制指令 $\{ A_t \}$

$$
J(x, \{ A_t \}) = \mathbb{E} \left [ \sum_{t=0}^{\infty} \gamma^{t} R(X_t, A_t) | X_0 = x  \right ]
$$

The optimal value function is defined, for each $x \in X$ as

> 最优值函数被定义，对于每个 $x \in X$ 

$$
 V^{*} (x) = \max_{A_t} J (x, \{At \})
$$

and verifies
> 并验证

$$
V^{*}(x) = \max_{a \in A} \sum_{y \in X} P_a(x, y) \left [  r(x, a, y) + \gamma V^{*}(y) \right ]
$$

From here we define the optimal Q-function, $Q^∗$ as

> 从这里，我们得到最优 Q 函数：$Q^∗$ 

$$
Q^{*}(x, a) = \sum_{y \in x} P_a (x, y) \left [  r(x, a, y) + \gamma V^{*}(y) \right ]
$$

The optimal Q-function is a fixed point of a contraction operator $\mathbf H$, defined for a generic function $q$: $X \times A \to \mathbb R$ as

> 最优 Q 函数是收缩算子 $\mathbf H$ 的不动点，定义为泛型函数 $q$： $X \times A \to \mathbb R$ 

$$
(\mathbf H_q)(x, a) = \sum_{y \in X} P_a(x, y) \left [  r(x, a, y) + \gamma \max_{b \in A} q(y, b)  \right ]
$$

This operator is a contraction in the sup-norm, i.e.,

> 这个算子是超范数的收缩，即

$$
\left \| \mathbf H_{q1} - \mathbf H_{q2} \right \| _{\infty} \leq \gamma \left \|  q_1 - q_2  \right \| _{\infty}. \ \ \ \ (1)
$$

To see this, we write
> 它可以经由下面的证明得到

$$
\left \| \mathbf H_{q1} - \mathbf H_{q2} \right \|_{\infty} = \\
= \max_{x, a} \left |  \sum_{y \in X}  P_a(x, y) \left [ r(x, a, y) + \gamma \max_{b \in A} q_1 (y, b) - r(x, a, y) - \gamma \max_{b \in A} q_2(y, b) \right ] \right | \\
= \max_{x, a} \gamma \left |  \sum_{y \in X} P_a(x, y) \left [  \max_{b \in A} q_1(y, b) - \max_{b \in A} q_2(y, b) \right ] \right | \\
\leq \max_{x, a} \gamma  \sum_{y \in X} P_a(x, y) \left |   \max_{b \in A} q_1(y, b) - \max_{b \in A} q_2(y, b)  \right | \\
\leq \max_{x, a} \gamma  \sum_{y \in X} P_a(x, y) \max_{z, b} \left | q_1(y, b) - q_2(y, b) \right | \\
= \max_{x, a} \gamma  \sum_{y \in X} P_a(x, y) \left \|  q_1 - q_2 \right \| \\
= \gamma \left \| q_1 - q_2  \right \|
$$

The Q-learning algorithm determines the optimal Q-function using point samples. Let $\pi$ be some random policy such that

> Q 学习算法使用点样本确定最佳 Q 函数。 让 $\pi$ 表示一些随机策略，于是

$$
\mathbb P_{\pi} \left [ A_t = a | X_t = x \right ]>0
$$

for all state-action pairs $(x, a)$. Let $\{ x_t \}$ be a sequence of states obtained following policy $\pi$, $\{ a_t \}$ the sequence of corresponding actions and $\{ r_t \}$ the sequence of obtained rewards. Then, given any initial estimate $Q_0$, Q-learning uses the following update rule:

> 对于所有状态-动作对 $(x, a)$。 令 $\{ x_t \}$ 是遵循策略 $\pi$ 获得的状态序列，$\{ a_t \}$ 是相应动作的序列，$\{ r_t \}$ 是获得奖励的序列。 然后，给定任何初始估计 $Q_0$，Q-learning 使用以下更新规则：

$$
Q_{t+1} (x_t,  a_t) = Q_t (x_t, a_t) + \alpha_t (x_t, a_t) \left [ r_t + \gamma \max_{b \in A} Q_t (x_{t+1}, b) − Q_t(x_t, a_t) \right ]
$$

where the step-sizes $\alpha_t (x, a)$ verify $0 \leq \alpha_t(x,a) \leq 1$. This means that, at the $(t + 1)^{th}$ update, only the component $(x_t, a_t)$ is updated.[^1]

> 其中步长 $\alpha_t (x, a)$ 为 $0 \leq \alpha_t(x,a) \leq 1$。 这意味着，在 $(t + 1)^{th}$ 更新时，只有组件 $(x_t, a_t)$ 被更新。[^1]

This leads to the following result.

> 这带来以下结果。

[^1]: There are variations of Q-learning that use a single transition tuple $(x, a, y, r)$ to perform updates in multiple states to speed up convergence, as seen for example in [2].

> **Theorem 1.** Given a finite MDP $(X , A, P, r)$, the Q-learning algorithm, given by the update rule
> $$
> Q_{t+1} (x_t, a_t) = Q_t (x_t, a_t) + \alpha_t (x_t, a_t) \left [ r_t + \gamma \max_{b \in A} Q_t (x_{t+1}, b) − Q_t (x_t, a_t) \right ], \ \ \ (2)
> $$
> converges w.p.1 to the optimal Q-function as long as
>
> $$
> \sum_{t} \alpha_t(x, a) = \infty, \ \  \sum_{t} \alpha_t^2 (x,a) < \infty \ \ \ \ \ \ \ \ \ (3)
> $$
> for all $(x, a) \in X \times A$.

Notice that, since $0 ≤ \alpha_t (x, a) < 1$, (3) requires that all state-action pairs be visited infinitely often.

> 请注意，由于 $0 ≤ \alpha_t (x, a) < 1$，(3) 这要求我们必须无限地频繁地访问所有状态-动作对。

To establish Theorem 1 we need an auxiliary result from stochastic approximation, that we promptly present.

> 为了确保定理1成立，我们需要一个随机逼近的辅助结果，我们于是提出。

> **Theorem 2**. The random process $\{ \Delta_t \}$ taking values in $\mathbb R^n$ and defined as > $$
> \Delta_{t+1} (x) = (1 − \alpha_t (x)) \Delta_t (x) + \alpha_t (x) F_t (x)
> $$
> converges to zero w.p.1 under the following assumptions: 
> * $0 \leq \alpha_t \leq 1, \ \sum_{t} \alpha_t (x) = \infty \ and \ \sum_{t} \alpha_t^2 (x)< \infty$; 
> * $\left \| E [ F_t (x) | F_t \right \|_W \leq \gamma \left \|  \Delta_t \right \|_W, with \ \gamma < 1$; 
> * $\mathbf{var} [ F_t(x) | F_t] \leq C (1 + \left \| \Delta_t \right \|_W^2), \ for \ C > 0$.


**Proof** See [1]. 
We are now in position to prove Theorem 1.

> 我们现在可以证明定理 1。

**Proof of Theorem 1** We start by rewriting (2) as

$$
Q_{t+1} (x_t,  a_t) = (1 − \alpha_t (x_t, a_t)) Q_t(x_t, a_t) + \alpha_t (x_t, a_t) \left [ r_t + \gamma \max_{b \in A} Q_t(x_{t+1}, b) \right ].
$$

Subtracting from both sides the quantity $Q^∗ (x_t, a_t)$ and letting
> 从两边同时减去 $Q^∗ (x_t, a_t)$ 并让

$$
\Delta_t(x, a) = Q_t(x, a) - Q^*(x, a)
$$

yields
> 得到

$$
\Delta_t(x, a) = (1 - \alpha_t (x_t, a_t)) \Delta_t (x_t, a_t)) + \\
\alpha_t(x, a) \left[  r_t + \gamma \max_{b \in A} Q_t(x_{t+1}, b) - Q^*(x_t. a_t) \right]
$$

If we write

$$
F_t (x, a) = r(x, a, X(x, a)) + \gamma \max_{b \in A} Q_t (y, b) − Q^∗(x, a)
$$

where $X (x, a)$ is a random sample state obtained from the Markov chain $(X , P_a)$, we have

> 其中$X(x,a)$是从马尔可夫链$(X,P_a)$获得的随机样本状态，我们有

$$
E \left[ F_t(x, a) | F_t \right] = \sum_{y \in X} P_a (x, y) \left[ r(x, a, y) + \gamma \max_{b \in A} Q_t(y, b) - Q^* (x, a) \right] \\
= (\mathbf{H} Q_t) (x, a) - Q^*(x, a)
$$

Using the fact that $Q^∗ = \mathbf{H}Q^∗$,

> 使用 $Q^∗ = \mathbf{H}Q^∗$ 的事实，

$$
\left \|  \mathbb{E} \left[ F_t(x, a) | F_t  \right] \right \| = (\mathbf{H}Q_t)(x, a) - ( \mathbf{H}Q^*)(x, a)
$$

It is now immediate from (1) that

> 现在直接从 (1) 得到

$$
\left \| \mathbb{E} \left[  F_t(x, a) | F_t \right] \right \|_{\infty} \leq \gamma \left \|  Q_t - Q^* \right \|_{\infty} = \gamma \left \|  \Delta_t \right \|_{\infty}
$$

Finally,

> 最终

$$
\mathbf{var} \left[ F_t(x) | F_t \right] = \\
= \mathbb{E} \left[ r(x, a, X(x, a)) + \gamma \max_{b \in A} Q_t(y, b) - Q^*(x, a) - (\mathbf{H}Q_t) (x, a) + Q^*(x, a)  \right]^2 \\
= \mathbb{E} \left[ r(x, a, X(x, a)) + \gamma \max_{b \in A} Q_t(y, b) - (\mathbf{H}Q_t)(x, a) \right]^2 \\
= \mathbf{var} \left[  r(x, a, X(x, a)) + \gamma \max_{b \in A} Q_t(y, b) | F_t \right]
$$

which, due to the fact that $r$ is bounded, clearly verifies

> 由于 $r$ 是有界的，这清楚地验证了

$$
\mathbf{var} [F_t(x) | F_t] \leq C(1 + \left | \Delta_t \right \|_W^2 )
$$

for some constant C. Then, by Theorem 2, $\Delta_t$ converges to zero w.p.1, i.e., $Q_t$ converges to $Q^*$ w.p.1.

> 对于某个常数 C。然后，根据定理 2，$\Delta_t$ 收敛到零 w.p.1，即 $Q_t$ 收敛到 $Q^*$ w.p.1。


# References
[1] Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. On the con- vergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6(6):1185–1201, 1994.
[2] Carlos Ribeiro and Csaba Szepesvári. Q-learning combined with spreading: Convergence and results. In Proceedings of the ISRF-IEE International Con- ference: Intelligent and Cognitive Systems (Neural Networks Symposium), pages 32–36, 1996.
